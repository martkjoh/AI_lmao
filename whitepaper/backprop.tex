\section*{Back propagation}
The partial derivatives of the cost function with respect to the weights in the last layer is given by
$$
    \frac{\partial C}{\partial w^{(l)}_{jk}} = \frac{\partial C}{\partial a^{(l)}_{j}} 
    \frac{\partial a^{(l)}_{j}}{\partial z^{(l)}_{j}} \frac{\partial z^{(l)}_{j}}{\partial w^{(i)}_{jk}}.
$$
We have
$$
    \frac{\partial C}{\partial a^{(l)}_{j}} = 2(a^{(l)}_{j} - y_j) 
$$
$$
    \frac{\partial a^{(l)}_{j}}{\partial z^{(l)}_{j}} = \frac{\partial f}{\partial x} = \frac{-\exp(x)}{(\exp(x) + 1)^2},
$$
and
$$
\frac{\partial z^{(l)}_{j}}{\partial w^{(l)}_{jk}} = a^{(l - 1)}_k,
$$
giving us this parital derivative. For subsequent layers, the derivative with respect to the activation is given by
$$
    \frac{\partial C}{\partial a^{(i - 1)}_{j}} = \sum_{k = 1}^{L_j}\frac{\partial C}{\partial a^{(i)}_{k}} 
    \frac{\partial a^{(i)}_{k}}{\partial z^{(i)}_{k}} \frac{\partial z^{(i)}_{k}}{\partial a^{(i -1)}_{k}},
$$
as the output depends on the activation of all the nodes in the former layer, the activation of that layer depnds on the activation of all the nodes in the layer before that, and som on. 
This can be calculated recursively. As before,
$$
    \frac{\partial a^{(i)}_{j}}{\partial z^{(i)}_{j}} = \frac{\partial f}{\partial x} = \frac{-\exp(x)}{(\exp(x) + 1)^2}
$$
and
$$
    \frac{\partial z^{(i)}_{j}}{\partial w^{(i)}_{jk}} = a^{(i - 1)}_k.
$$
To find the derivative with respect to the weight, we implicilty differentiate $a^{(i)}_{j}$,
$$ 
    \frac{\partial a^{(i)}_{j}}{\partial w^{(i)}_{jk}} = \frac{\partial}{w^{(i)}_{jk}}f\big(w^{(i)}_{jk} a^{(i-1)}_k + b_j \big) = \frac{\partial f}{ \partial z^{(i)}_{j}} \frac{\partial z^{(i)}_{j}}{ \partial a^{(i-1)}_{j}} = f^\prime (z)a^{(i-1)}_{j}.
$$
This gives
$$
    \frac{\partial C}{\partial w^{(i - 1)}_{jk}} = \frac{\partial C}{\partial a^{(i - 1)}_{j}} \frac{\partial a^{(i - 1)}_{j}}{\partial w^{(i - 1)}_{jk}} = \frac{\partial C}{\partial a^{(i - 1)}_{j}} f^\prime (z)a^{(i-1)}_{j}.
$$

To find the full gradient, we also need the partial derivatives with respect to the biases. These can now be easily found as,
$$
    \frac{\partial C}{\partial b^{(l)}_{j}} = \frac{\partial C}{\partial a^{(l)}_{j}} 
    \frac{\partial a^{(l)}_{j}}{\partial z^{(l)}_{j}} \frac{\partial z^{(l)}_{j}}{\partial b^{(l)}_{j}} = \frac{\partial C}{\partial a^{(l)}_{j}} \frac{\partial a^{(l)}_{j}}{\partial z^{(l)}_{j}}
$$
and
$$
    \frac{\partial C}{\partial b^{(i)}_{j}} = \sum_{k = 1}^{L_j}\frac{\partial C}{\partial a^{(i)}_{k}} 
    \frac{\partial a^{(i)}_{k}}{\partial z^{(i)}_{k}} \frac{\partial z^{(i)}_{k}}{\partial b^{(i)}_{j}} = \sum_{k = 1}^{L_j}\frac{\partial C}{\partial a^{(i)}_{k}}
    \frac{\partial a^{(i)}_{k}}{\partial z^{(i)}_{k}}.
$$

The backpropagating algorithm therefore consists of 
\begin{enumerate}
    \item Find $\partial C / \partial w^{(l)}_{jk}$ and $\partial C / \partial b^{(l)}_{j}$.
    \item Loop backwards through the neural net to find Find $\partial C / \partial a^{(i)}_{j}$ and $\partial C / \partial b^{(i)}_{j}$ for $i \in \{1 ... l-1 \}$.
    \item Use $\partial C / \partial a^{(i)}_{j}$ to find $\partial C / \partial w^{(i)}_{jk}$-
\end{enumerate}
This wil result in the gradient of $C$ with respect to the weights and biases, 
$$
    \nabla C =
    \begin{bmatrix}
        \partial C / \partial w^{(1)}_{11} \\
        \vdots \\
        \partial C/\partial w^{(l)}_{L_lL_l} \\
        \vspace{1px}\\
        \partial C/\partial b^{(1)}_{j} \\
        \vdots \\
        \partial C /\partial b^{(l)}_{L_l} \\
    \end{bmatrix}
$$
